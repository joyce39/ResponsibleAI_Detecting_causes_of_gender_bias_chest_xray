{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAI Exercise 1: Algorithmic Fairness ‚öñÔ∏è\n",
    "\n",
    "# Important information\n",
    "This exercise is part of the RAI course (02517 - **Responsible AI: Algorithmic Fairness and Explainability**) at the Technical University of Denmark (DTU). You can find more details about the course [here](https://kurser.dtu.dk/course/02517). This specific version is for the Fall 2024 semester.\n",
    "\n",
    "If you have any questions related to this notebook, feel free to reach out to Nina Weng at *ninwe@dtu.dk*.\n",
    "\n",
    "**Credits**:  \n",
    "We thank:\n",
    "* NIH dataset team for collecting such dataset [link to the paper](https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf);\n",
    "* Authors from [this paper](https://link.springer.com/chapter/10.1007/978-3-031-45249-9_14) for providing the splits;\n",
    "* [Memes generator webpage imgflip](https://imgflip.com/) for all the excellent Memes template.\n",
    "\n",
    "\n",
    "# PART 1: Fairness assessment and Bias mitigation using Fairlearn\n",
    "\n",
    "The goal of this exercise is to learn how to use [Fairlearn](https://fairlearn.org/) to approach basic fairness assessments and apply post-processing bias mitigation methods. \n",
    "\n",
    "Fairlearn is an open-source Python package originally developed by Microsoft Research. Since 2021, it has become completely community-driven. For more information about Fairlearn, you can visit [this page]((https://fairlearn.org/v0.10/about/index.html)). \n",
    "\n",
    "Although Fairlearn is likely the most well-developed package targeting fairness issues, it has its limitations. The most notable limitation, that might need to be mentioned at very beginning for this exercise, is that Fairlearn is primarily designed for tabular data ([this page](https://fairlearn.org/main/faq.html) under question: *Does Fairlearn work for image and text data?*). Therefore, when working with other types of data, such as image data, unexpected issues may arise. Fortunate enough, there are workarounds for most of these issues, which will be discussed later in this exercise.\n",
    "\n",
    "While Fairlearn is a good resource and offers an easy approach for learning fairness concepts and handling lighter tasks, it may not be the best solution for researchers working extensively in this area. Keep that in mind :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Objective of this Exercise (PART 1)\n",
    "By the end of this exercise, you should be able to:\n",
    "\n",
    "* Assess fairness using Fairlearn with provided predictions/probabilities and target labels. This includes calculating metrics, generating ROC curves, and interpreting their meaning.\n",
    "* Apply post-processing bias mitigation techniques using Fairlearn, and clearly understand and explain the outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./support4notebook/getstarted.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset: Chest X-ray and lung/heart related disease\n",
    "\n",
    "\n",
    "In this exercise, we will use a chest X-ray dataset and a basic deep learning model as the setup. It requires the following:\n",
    "\n",
    "* Download the dataset/metadata/pretrained ResNet model. Note that in this exercise, we only use part of the data, and details are listed below. The full dataset can be found [here](https://nihcc.app.box.com/v/ChestXray-NIHCC). (For students in the class, you can find a download link on DTU Learn. For those not in the class, you can find the pre-processing scripts in [this repository](https://github.com/nina-weng/detecting_causes_of_gender_bias_chest_xrays).)\n",
    "* After downloading the materials, put the `NIH_train_val_test_rs0_f50.csv` under `./datafiles/`; and `nih_pneumothorax.pth` under `./pretrained_model/`.\n",
    "* Prepare your virtual environment: \n",
    "`conda env create -f env.yml`\n",
    "\n",
    "\n",
    "### Chest xray samples\n",
    "We use [NIH chest xray dataset](https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community) in this exercise. Let‚Äôs take a closer look at the dataset. </br>\n",
    "\n",
    "This dataset contains 108,948 images from 32,717 patients, each labeled with one of 14 types of lung or heart-related diseases/symptoms. For detailed information on each disease, you can find explanations [here](https://nihcc.app.box.com/v/ChestXray-NIHCC/file/220660789610).\n",
    "\n",
    "For simplicity, we will use only one sample per patient and preprocess the images to a size of 224x224. Both the dataset and the metadata (in CSV format) are available. The dataset split is also specified in the metadata under the column 'split'.\n",
    "\n",
    "Note: The split was designed for a different task, which required a larger test set than usual. As a result (as you‚Äôll notice), the test set is relatively large (around 8k for training, 2k for validation, and 8k for testing). If you find that the mitigation or prediction process takes too long, feel free to downsample the test set. Just ensure you validate that the proportions of samples across different sensitive groups and disease labels remain roughly consistent with the original test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: change this to your data directory\n",
    "datadir = \"/Users/joyceabisaleh/Desktop/Responsible AI/detecting_causes_of_gender_bias_chest_xrays-master\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from fairlearn.metrics import MetricFrame, selection_rate,false_positive_rate,true_positive_rate,false_negative_rate,true_negative_rate\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "\n",
    "from train.model import ResNet\n",
    "from train.prediction import validate\n",
    "from analysis.plot import plot_roc_simple\n",
    "from analysis.tools import from_loader_to_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look of some samples, with filter query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pth = datadir + '/NIH_part/'\n",
    "metadata_csv ='./datafiles/NIH_train_val_test_rs0_f50.csv'\n",
    "metadata = pd.read_csv(metadata_csv)\n",
    "\n",
    "# display(metadata.head(5))\n",
    "\n",
    "# randomly choose some samples from PAD-UFES-20\n",
    "def show_random_images(datadir,metadata,seed=None,filter_str=None,num_sample=5):\n",
    "    fig = plt.figure(figsize=(num_sample*3, 3),dpi=200)\n",
    "    files = os.listdir(datadir)\n",
    "    if filter_str:\n",
    "        metadata = metadata.query(filter_str)\n",
    "        # display(metadata.head(5))\n",
    "    if seed is not None:\n",
    "        random_sample = metadata.sample(n=num_sample, random_state=seed)\n",
    "    else: random_sample = metadata.sample(n=num_sample)\n",
    "    \n",
    "    for i in range(len(random_sample)):\n",
    "        row = random_sample.iloc[i]\n",
    "        disease = row['Pneumothorax']\n",
    "        sex = row['Patient Gender']\n",
    "\n",
    "        img = mpimg.imread(datadir + row['Image Index'])\n",
    "        ax = fig.add_subplot(1, len(random_sample), i + 1)\n",
    "        # add diagnosis as subtitle\n",
    "        ax.set_title(f'{disease=},{sex=}')\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(f'{filter_str}')\n",
    "    plt.show()\n",
    "\n",
    "show_random_images(dataset_pth,metadata=metadata,seed=42,filter_str='Pneumothorax==1 and `Patient Gender`==\"F\"',num_sample=5)\n",
    "show_random_images(dataset_pth,metadata=metadata,seed=42,filter_str='Pneumothorax==1 and `Patient Gender`==\"M\"',num_sample=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic statistics\n",
    "\n",
    "We can also take a look at the distribution of some sensitive attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution_by_value(metadata, column_name):\n",
    "    if isinstance(column_name, str):\n",
    "        nan_count = metadata[column_name].isna().sum()\n",
    "        nan_series = pd.Series([nan_count], index=['NaN'])\n",
    "        counts_ = metadata[column_name].value_counts().sort_index()\n",
    "        counts_with_nan = pd.concat([counts_, nan_series])\n",
    "\n",
    "        counts_with_nan.plot(kind='bar',title='Distribution of {}'.format(column_name))\n",
    "        plt.ylabel('count')\n",
    "    elif isinstance(column_name, list):\n",
    "        fig, axes = plt.subplots( 1, len(column_name), figsize=( len(column_name)*4,3),dpi=200)\n",
    "        for i,col in enumerate(column_name):\n",
    "            nan_count = metadata[col].isna().sum()\n",
    "            nan_series = pd.Series([nan_count], index=['NaN'])\n",
    "            counts_ = metadata[col].value_counts().sort_index()\n",
    "            counts_with_nan = pd.concat([counts_, nan_series])\n",
    "\n",
    "            counts_with_nan.plot(kind='bar',title='Distribution of {}'.format(col),ax=axes[i])\n",
    "            axes[i].set_ylabel('count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "metadata['age_range'] = pd.cut(metadata['Patient Age'], bins=[0,10,20,30,40,50,60,70,80,90,100], right=False)\n",
    "plot_distribution_by_value(metadata, ['age_range','Patient Gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that, for this specific split, we maintain an equal number of male and female samples. This balance is consistent across all three splits: training, validation, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÉ Further Reading:\n",
    "If you're interested, here are some studies that explore potential biases and confounders in chest xray datasets:\n",
    "* [Lauren Oakden-Rayner: Exploring the ChestXray14 dataset: problems](https://laurenoakdenrayner.com/2017/12/18/the-chestxray14-dataset-problems/)\n",
    "* [Amelia Jim√©nez-S√°nchez et al.: Detecting Shortcuts in Medical Images -- A Case Study in Chest X-rays](https://arxiv.org/abs/2211.04279)\n",
    "* [Judy Wawira Gichoya et al.: AI recognition of patient race in medical imaging: a modelling study](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(22)00063-2/fulltext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fairness assessment\n",
    "**Recap of Key Concepts**:\n",
    "\n",
    "In th class, we have learned:\n",
    "* The three key criteria for fairness assessment. What are they?\n",
    "* Evaluation metrics corresponding to each criterion.\n",
    "* ROC curves.\n",
    "\n",
    "For this exercise, we‚Äôve provided a pre-trained ResNet classifier for Part 1, where the disease label is `Pneumothorax` and the sensitive attribute is `sex` (in metadata, you can get the binarized sex label from column `sex label`, where 0 represents female and 1 represents male). However, feel free to train your own model if you'd like.\n",
    "\n",
    "\n",
    "\n",
    "### Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the pretrained model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(' device:', device)\n",
    "\n",
    "ds_name = 'NIH'\n",
    "\n",
    "# load the model\n",
    "lr=1e-6\n",
    "pretrained = True\n",
    "model_scale = '18'\n",
    "num_epochs =20\n",
    "img_size = (1, 224, 224)\n",
    "\n",
    "#def load(f, map_location='cpu', pickle_module=pickle, **pickle_load_args):\n",
    "classifier = ResNet(num_classes=1, lr=lr, pretrained=pretrained, model_scale=model_scale, in_channel=img_size[0])\n",
    "classifier.load_state_dict(torch.load('./pretrained_model/nih_pneumothorax.pth', map_location=torch.device('cpu')))\n",
    "classifier.to(device)\n",
    "\n",
    "classifier.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_at = './pretrained_models/'\n",
    "\n",
    "img_size = (1,224,224)\n",
    "batch_size = 16\n",
    "\n",
    "csv_pth = './datafiles/NIH_train_val_test_rs0_f50.csv' if ds_name == 'NIH' else None\n",
    "\n",
    "disease_label = 'Pneumothorax' \n",
    "sensitive_label = 'sex'\n",
    "augmentation = False\n",
    "\n",
    "from train.train_chestxray import create_datasets\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = create_datasets(dataset_pth, \n",
    "                                                               ds_name,\n",
    "                                                               csv_pth, \n",
    "                                                               image_size=img_size, \n",
    "                                                               device=device,\n",
    "                                                               disease_label = disease_label,\n",
    "                                                               sensitive_label = sensitive_label,\n",
    "                                                               augmentation=augmentation)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # we dont need it here\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the results for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lab, test_pred, test_prob, test_a= validate(classifier, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess fairness using Fairlearn\n",
    "#### A simple example first\n",
    "\n",
    "Fairlearn provides the `fairlearn.metrics.MetricFrame` class to help with this quantification. \n",
    "\n",
    "Given: \n",
    "<pre>\n",
    "y_true = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "y_pred = [0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0]\n",
    "sf_data = ['b', 'b', 'a', 'b', 'b', 'c', 'c', 'c', 'a',\n",
    "           'a', 'c', 'a', 'b', 'c', 'c', 'b', 'c', 'c']\n",
    "</pre>\n",
    "           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./support4notebook/exercise_time.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try: \n",
    "* measure: recall, selection rate and false positive rate for *each group*;\n",
    "* plot the above result out; ([Hint](https://fairlearn.org/main/user_guide/assessment/plotting.html))\n",
    "* measure the difference in eqaulized odd between different groups;\n",
    "\n",
    "\n",
    "Hint: The documentation page of [MetricFrame](https://fairlearn.org/main/api_reference/generated/fairlearn.metrics.MetricFrame.html#fairlearn.metrics.MetricFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, confusion_matrix\n",
    "\n",
    "# Create a function to calculate recall, selection rate, and FPR by group\n",
    "def calculate_group_metrics(true_labels, predictions, group_labels):\n",
    "    # Convert lists to NumPy arrays for easier indexing\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "    group_labels = np.array(group_labels)\n",
    "    \n",
    "    group_metrics = {}\n",
    "    groups = np.unique(group_labels)  # Get unique group labels (e.g., male, female)\n",
    "\n",
    "    for group in groups:\n",
    "        # Get the indices of the current group\n",
    "        group_idx = np.where(group_labels == group)\n",
    "        \n",
    "        # Extract true labels and predictions for this group\n",
    "        true_group = true_labels[group_idx]\n",
    "        pred_group = predictions[group_idx]\n",
    "\n",
    "        # Recall (True Positive Rate)\n",
    "        recall = recall_score(true_group, pred_group)\n",
    "        \n",
    "        # Selection Rate\n",
    "        selection_rate = np.mean(pred_group)\n",
    "\n",
    "        # False Positive Rate (FPR)\n",
    "        tn, fp, fn, tp = confusion_matrix(true_group, pred_group).ravel()\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "        # Store the metrics for the group\n",
    "        group_metrics[group] = {\n",
    "            \"Recall\": recall,\n",
    "            \"Selection Rate\": selection_rate,\n",
    "            \"False Positive Rate\": fpr\n",
    "        }\n",
    "    \n",
    "    return group_metrics\n",
    "\n",
    "# Example usage\n",
    "#test_lab, test_pred, test_prob, test_a = validate(classifier, test_loader, device=device)\n",
    "\n",
    "# Calculate metrics by group (test_a is the sensitive attribute like gender)\n",
    "group_metrics = calculate_group_metrics(test_lab, test_pred, test_a)\n",
    "\n",
    "# Print metrics for each group\n",
    "for group, metrics in group_metrics.items():\n",
    "    print(f\"Group: {group}\")\n",
    "    print(f\"  Recall: {metrics['Recall']:.2f}\")\n",
    "    print(f\"  Selection Rate: {metrics['Selection Rate']:.2f}\")\n",
    "    print(f\"  False Positive Rate: {metrics['False Positive Rate']:.2f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now measure the fairness metrics for our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./support4notebook/exercise.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to calculate False Positive Rate (FPR)\n",
    "def false_positive_rate(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    return fpr\n",
    "\n",
    "# Define a custom function to calculate Selection Rate\n",
    "def selection_rate(y_true, y_pred):\n",
    "    return np.mean(y_pred)\n",
    "\n",
    "# Define a custom function to calculate Recall\n",
    "def recall_metric(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred)\n",
    "    \n",
    "\n",
    "mf = MetricFrame(metrics={\n",
    "                    'accuracy': accuracy_score,\n",
    "                    'Recall': recall_metric,\n",
    "                    'Selection Rate': selection_rate,\n",
    "                    'False Positive Rate': false_positive_rate\n",
    "                },\n",
    "                 y_true=test_lab,\n",
    "                 y_pred=test_pred,\n",
    "                 sensitive_features=test_a)\n",
    "\n",
    "print(\"Test set fairness metrics (before mitigation):\")\n",
    "print(mf.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the ROC curve\n",
    "\n",
    "Here we provide the funtion `plot_roc_simple` to draw the ROC curve for each groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_simple(test_lab, test_prob, test_a, test_pred,\n",
    "        sensitive_attribute_name = 'sex',\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí• Exercise and discusssion:\n",
    "* What do you see from the metrics and the ROC curve?\n",
    "\n",
    "* Performance Differences:\n",
    "\n",
    "There is a noticeable difference in the ROC curves between the two groups (0.0 for Female, 1.0 for Male). The AUC for females (0.74) is lower than the AUC for males (0.80), suggesting that the model performs better at distinguishing between positive and negative cases for males compared to females.\n",
    "\n",
    "Fairness Concerns:\n",
    "\n",
    "The difference in AUC indicates that there is an imbalance in model performance, soo there is potential unfairness towards the female group. The true positive rate (TPR) for males appears to be consistently higher across most false positive rates, which means the model is more likely to correctly identify positives for males than for females.\n",
    "\n",
    "* Accuracy:\n",
    "\n",
    "The model has an accuracy of 0.769 for the female group, it presents a higher accuracy of 0.849 for the male group. This means that the model is generally more accurate in predicting outcomes for males.\n",
    "\n",
    "* Recall:\n",
    "\n",
    "The recall for the mfeale group is 0.619, so the model identifies about 61.9% of all actual positives correctly in the female group.\n",
    "The recall is 0.556 for the male group meaning only  55.6% of actual positives are correctly identified for males.\n",
    "\n",
    "This suggests a weaker sensitivity towards positives in the male group despite the higher observed accuracy.\n",
    "\n",
    "* Selection Rate:\n",
    "\n",
    "The selection rate for the female group is 0.244, it is lower for the male group at 0.156. This indicates that the model is less likely to predict positive outcomes for individuals in the male group.\n",
    "\n",
    "* False Positive Rate (FPR):\n",
    "\n",
    "The FPR for the female group is 0.222. It is lower for the male group at 0.138. This means the model is less likely to incorrectly label negative cases as positive in the male group, aligning with the higher accuracy and AUC noted.\n",
    "\n",
    "The differences observed through the AUC curve in addition to recall, selection rate, and FPR raise questions about the fairness of the model. \n",
    "\n",
    "While the model performs better overall for the male group in terms of accuracy and discriminatory power (AUC)The lower recall and selection rate for the male group indicate that the model has higher standards for classifying positives in this group and might classify only the most obvious cases as positive. Adjusting the decision threshold mighthelp balance the performance across the two group groups, improving fairness.\n",
    "This would be done by incresing recall and classifying more true positives correctly. It is important to make sure this does not increase the false positive rate too much. \n",
    "\n",
    "\n",
    "* Try to measure and desribe the fairness wrt the 3 creterias we learned from class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairness analysis with respect to the three criteria\n",
    "def fairness_analysis(metrics_frame):\n",
    "    print(\"\\nFairness Analysis:\")\n",
    "    \n",
    "    # Demographic Parity: Check if selection rates are similar across groups\n",
    "    selection_rates = metrics_frame.by_group['Selection Rate']\n",
    "    print(f\"Demographic Parity - Selection Rates by Group:\\n{selection_rates}\")\n",
    "    \n",
    "    # Equalized Odds: Check if recall (TPR) and FPR are similar across groups\n",
    "    recalls = metrics_frame.by_group['Recall']\n",
    "    fprs = metrics_frame.by_group['False Positive Rate']\n",
    "    print(f\"\\nEqualized Odds - Recall and False Positive Rate by Group:\\nRecall:\\n{recalls}\\nFalse Positive Rate:\\n{fprs}\")\n",
    "    \n",
    "    # Equal Opportunity: Check if recall (TPR) is similar across groups\n",
    "    print(f\"\\nEqual Opportunity - Recall by Group:\\n{recalls}\")\n",
    "\n",
    "fairness_analysis(mf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Demographic Parity, Equalized Odds, and Equal Opportunity.\n",
    "*Demographic Parity - because it treats the two groups differently in terms of selection rate. In practice, this might mean that females are receiving more positive diagnoses than males, which could lead to unfair treatment depending on the context.\n",
    "\n",
    "*Equalized Odds is satisfied if both the recall (true positive rate) and false positive rate (FPR) are similar across groups, but:\n",
    "\n",
    "*Recall is higher for females compared to males, meaning that the model is better at identifying true positives for females. FPR is also higher for females, indicating that the model is more likely to produce false positives for females than for males.\n",
    "\n",
    "*The differences in both recall and FPR mean that the model fails to satisfy Equalized Odds, as its behavior varies between the two groups. This could mean that the model is less reliable for males, possibly missing more true positive cases and making fewer incorrect positive predictions.\n",
    "\n",
    "* Equal Opportunity requires that the recall be similar across groups. The recall for females is higher than for males, indicating that the model is better at correctly identifying positive cases for females. This suggests that the model provides unequal opportunities for males, as they are less likely to receive a true positive prediction compared to females."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bias mitigation using Fairlearn\n",
    "\n",
    "### Recall from the class\n",
    "* what kinds of mitigation methods have we learned?\n",
    "\n",
    "In this exercise, we will try to use the one pf the post-preprocessing bias mitiagtion method Fairlearn provided: **Threshold Optimization**, to implement the bias mitigation steps. \n",
    "\n",
    "### The theory of Threshold Optimization\n",
    "\n",
    "\n",
    "The idea could be simply visualized as below (figure from the [original paper](https://arxiv.org/pdf/1610.02413)):  \n",
    "![](./support4notebook/threshold_op.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where blue/green represent two sensitive groups, any points in the overlapping region meet the requirement of equalized odds:\n",
    "\n",
    "$$\n",
    "\\gamma_0(\\hat{Y}) = \\gamma_1(\\hat{Y}),\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\gamma_a (\\hat{Y}) = \\left(Pr(\\hat{Y} = 1 | A = a, Y = 0), Pr(\\hat{Y} = 1 | A = a, Y=1)\\right).\n",
    "$$\n",
    "\n",
    "The goal of the threshold optimizer is to find the point in the overlapping region that optimizes the objective function, such as balanced accuracy.\n",
    "\n",
    "To achieve this, **randomization** is introduced. The idea is starightforward: any point under the ROC curve can be estimated by weighting two points on the ROC curve (which could be achieved by simply thresholding); or in another word, a new decision threshold $T_a$ can be a randomized mixture of two decision thresholds $\\underline{t}_a$ and $\\overline{t}_a$.\n",
    "\n",
    "(See the figure below, which is from [this paper](https://arxiv.org/abs/2202.08536)).\n",
    "\n",
    "![Randomization Figure](./support4notebook/randomization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üìÉ Further Reading:\n",
    "* [Fairlearn *ThresholdOptimizier* page](https://fairlearn.org/v0.5.0/api_reference/fairlearn.postprocessing.html).\n",
    "* The original paper (See section 3): [Equality of opportunity in supervised learning](https://arxiv.org/pdf/1610.02413).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü™Ñ Trick: A fake classifier class\n",
    "\n",
    "Fairlearn has some limitations when implementing the `ThresholdOptimizer` method. To work around these issues, a fake classifier is provided to bypass minor problems. If you use the provided classifier, this fake class should work just as well.\n",
    "\n",
    "However, if you're curious about what went wrong or want to use your own classifier, please read below:\n",
    "\n",
    "* **Problem 1: `estimator` in `ThresholdOptimizer` only accepts 2D input (tabular data).** This doesn‚Äôt make sense for post-processing mitigation methods, as the only relevant aspect here is the prediction scores from the test set. The classifier itself and the input data are irrelevant when optimizing the threshold.\n",
    "  \n",
    "* **Problem 2: The `prefit` parameter checks whether the model has been fitted in a simplistic way, leading to errors.** You can read more about this fit check function [here](https://scikit-learn.org/stable/modules/generated/sklearn.utils.validation.check_is_fitted.html).\n",
    "  \n",
    "* **Problem 3: It requires the prediction function to return scores for both classes in binary classification.** This might be an issue if your classifier only provides the probability for class 1.\n",
    "\n",
    "**How we solve this**: We create a fake classifier that accepts 2D input and reshapes it back to the original image size before feeding it into the prediction function. We trick the fit check by defining a fake variable and manually modify the output of the prediction function to include both classes if it only returns the probability for class 1.\n",
    "\n",
    "Note: If you trained your own classifier, you will need to implement a custom fake classifier yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class FakeClassifierInput2D(BaseEstimator,ClassifierMixin):\n",
    "    '''\n",
    "    Fake classifier that takes 2D input, with pre-trained model that does not take 2D input data\n",
    "    '''\n",
    "    def __init__(self,model, img_size):\n",
    "        self.model = model\n",
    "        self.img_size = img_size\n",
    "        self.input_from_2D_func = lambda x: torch.reshape(x,(-1,)+self.img_size)\n",
    "        # self.input_from_2D_func = input_from_2D_func\n",
    "        self.fit_ = True # fake the check_fit function inside the fairlearn library\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Do not need to fit\n",
    "        return\n",
    "\n",
    "    def predict(self, X_2D, ):\n",
    "        #assert len(X_2D.shape) == 2\n",
    "        #X = self.input_from_2D_func(X_2D)\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X_2D):\n",
    "        X = self.input_from_2D_func(X_2D)\n",
    "        return self.model.predict_proba(X)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, a_test = from_loader_to_tensor(test_loader,device)\n",
    "X_val, y_val, a_val = from_loader_to_tensor(val_loader,device)\n",
    "\n",
    "X_test_2D = torch.reshape(X_test,(X_test.shape[0],-1))\n",
    "X_val_2D = torch.reshape(X_val,(X_val.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_fake = FakeClassifierInput2D(model=classifier.to('cpu'),\n",
    "                                       img_size = img_size,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "#fake classifier already fitted\n",
    "\n",
    "\n",
    "\n",
    "postprocessor = ThresholdOptimizer(\n",
    "    estimator=classifier_fake,\n",
    "    constraints=\"equalized_odds\",\n",
    "    objective = \"balanced_accuracy_score\",\n",
    "    #constraints=\"false_negative_rate_parity\",\n",
    "    prefit=True,\n",
    "    predict_method=\"predict_proba\"\n",
    ")\n",
    "\n",
    "postprocessor.fit(X_val_2D, y_val, sensitive_features=a_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from fairlearn.postprocessing import plot_threshold_optimizer\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plot_threshold_optimizer(postprocessor, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the new threshold for the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_fair_test = postprocessor.predict(X_test, sensitive_features=a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_pred = postprocessor.predict(X_test_2D, sensitive_features=a_test)\n",
    "y_pred_fair_test = postprocessor.predict(X_test, sensitive_features=a_test)\n",
    "\n",
    "mf = MetricFrame(metrics={\n",
    "                    'accuracy': accuracy_score,\n",
    "                    'Recall': recall_metric,\n",
    "                    'Selection Rate': selection_rate,\n",
    "                    'False Positive Rate': false_positive_rate\n",
    "                },\n",
    "                 y_true=X_test_2D,\n",
    "                 y_pred=y_pred_fair_test,\n",
    "                 sensitive_features=test_a)\n",
    "\n",
    "print(\"Test set fairness metrics (before mitigation):\")\n",
    "print(mf.by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_simple(test_lab, test_prob, test_a, y_pred_fair_test,\n",
    "        sensitive_attribute_name = 'sex',\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To find out how the prediction come from (the new threshold $T_a$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "threshold_rules_by_group = to.interpolated_thresholder_.interpolation_dict\n",
    "print(json.dumps(threshold_rules_by_group, default=str, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí• Exercise and Discussion:\n",
    "* Can you write down the new threshold function? ([Hint](https://fairlearn.org/v0.10/user_guide/mitigation/postprocessing.html#postprocessing))\n",
    "* Compare the results. What do you observe, and does this model seem fair to you?\n",
    "* Hint: After optimization, you may notice that accuracy (or other metrics) is more balanced between groups. However, the overall accuracy (or other metrics of interest) may decrease for both groups. Do you think this is still a good or acceptable solution?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./support4notebook/dilemma.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Potential pitfall: Algorithmic fairness in the presence of label noise\n",
    "As mentioned in class, it is easy to diagnose algorithmic bias. This is, however, only true if we have access to correct target labels for the test set. In this part of the project, we will simulate a situation where our test set ground truth target labels are incorrect in a biased way: You will simulate overdiagnosis among male individuals, by manually distorting some of their labels. Next, you will analyze how this affects the diagnosis and mitigation of algorithmic bias.\n",
    "\n",
    "## Write a script to distort the labels for the male individuals according to the following recipe:\n",
    "* Please create a new set of distorted target labels\n",
    "* Initialize these as identical to the supplied target labels\n",
    "* Manually distort them by flipping 30% of the healthy labels for male individuals to diseased. These should be selected at random.\n",
    "\n",
    "## Now repeat your analysis from Part 1 for your classifier from Part 1 using the distorted labels. \n",
    "You don‚Äôt need to retrain the classifier ‚Äì you will only repeat the diagnosis and mitigation parts.\n",
    "\n",
    "* Diagnose algorithmic bias with respect to your distorted labels. Do your conclusions change?\n",
    "* Mitigate algorithmic bias with respect to your distorted labels. Following this, repeat your diagnostic pipeline both with respect to your distorted and original labels. What do you see? Did mitigation ensure improved fairness with respect to the distorted labels? What happened with respect to the actual (original) labels? Is the mitigated algorithm actually fair?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Assuming test_lab and test_a are numpy arrays, and you have two groups (e.g., 'male' and 'female')\n",
    "test_lab_new = []\n",
    "groups = np.unique(test_a)  # Unique sensitive groups\n",
    "test_lab = np.array(test_lab)  # Convert test_lab to a numpy array for easier manipulation\n",
    "\n",
    "# Split test_lab by group and store in test_lab_new\n",
    "for group in groups:\n",
    "    group_idx = np.where(test_a == group)  # Get indices for the group\n",
    "    true_group = test_lab[group_idx]  # Extract corresponding labels for the group\n",
    "    test_lab_new.append(true_group)  # Append the extracted group labels to the list\n",
    "\n",
    "# Assuming group 1 (index 1) is male, modify that group (male_distorted)\n",
    "male_distorted = test_lab_new[1]  # Assuming male is at index 1 (adjust accordingly)\n",
    "\n",
    "# Get indices where the values are 0 in the male_distorted array\n",
    "zero_indices = [i for i, x in enumerate(male_distorted) if x == 0]\n",
    "\n",
    "# Randomly sample 30 indices to flip\n",
    "random_indices_to_flip = random.sample(zero_indices, 30)\n",
    "\n",
    "# Flip values at the selected indices\n",
    "for idx in random_indices_to_flip:\n",
    "    male_distorted[idx] = 1\n",
    "\n",
    "# Rebuild the original test_lab array with the changes in male_distorted\n",
    "# Now, you replace the unchanged parts back into the main test_lab array\n",
    "\n",
    "# Initialize an empty list to rebuild the test_lab\n",
    "distorted_test_lab = np.copy(test_lab)\n",
    "\n",
    "# Replace the 'male' group with the modified male_distorted array\n",
    "male_idx = np.where(test_a == groups[1])  # Assuming 'male' is group[1], adjust if needed\n",
    "distorted_test_lab[male_idx] = male_distorted\n",
    "\n",
    "print(\"Modified test_lab with changes to male group:\")\n",
    "print(distorted_test_lab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate metrics by group (test_a is the sensitive attribute like gender)\n",
    "group_metrics = calculate_group_metrics(distorted_test_lab, test_pred, test_a)\n",
    "\n",
    "# Print metrics for each group\n",
    "for group, metrics in group_metrics.items():\n",
    "    print(f\"Group: {group}\")\n",
    "    print(f\"  Recall: {metrics['Recall']:.2f}\")\n",
    "    print(f\"  Selection Rate: {metrics['Selection Rate']:.2f}\")\n",
    "    print(f\"  False Positive Rate: {metrics['False Positive Rate']:.2f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to calculate False Positive Rate (FPR)\n",
    "def false_positive_rate(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    return fpr\n",
    "\n",
    "# Define a custom function to calculate Selection Rate\n",
    "def selection_rate(y_true, y_pred):\n",
    "    return np.mean(y_pred)\n",
    "\n",
    "# Define a custom function to calculate Recall\n",
    "def recall_metric(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred)\n",
    "    \n",
    "\n",
    "mf = MetricFrame(metrics={\n",
    "                    'accuracy': accuracy_score,\n",
    "                    'Recall': recall_metric,\n",
    "                    'Selection Rate': selection_rate,\n",
    "                    'False Positive Rate': false_positive_rate\n",
    "                },\n",
    "                 y_true=distorted_test_lab,\n",
    "                 y_pred=test_pred,\n",
    "                 sensitive_features=test_a)\n",
    "\n",
    "print(\"Test set fairness metrics (before mitigation):\")\n",
    "print(mf.by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_simple(distorted_test_lab, test_prob, test_a, test_pred,\n",
    "        sensitive_attribute_name = 'sex',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairness analysis with respect to the three criteria\n",
    "def fairness_analysis(metrics_frame):\n",
    "    print(\"\\nFairness Analysis:\")\n",
    "    \n",
    "    # Demographic Parity: Check if selection rates are similar across groups\n",
    "    selection_rates = metrics_frame.by_group['Selection Rate']\n",
    "    print(f\"Demographic Parity - Selection Rates by Group:\\n{selection_rates}\")\n",
    "    \n",
    "    # Equalized Odds: Check if recall (TPR) and FPR are similar across groups\n",
    "    recalls = metrics_frame.by_group['Recall']\n",
    "    fprs = metrics_frame.by_group['False Positive Rate']\n",
    "    print(f\"\\nEqualized Odds - Recall and False Positive Rate by Group:\\nRecall:\\n{recalls}\\nFalse Positive Rate:\\n{fprs}\")\n",
    "    \n",
    "    # Equal Opportunity: Check if recall (TPR) is similar across groups\n",
    "    print(f\"\\nEqual Opportunity - Recall by Group:\\n{recalls}\")\n",
    "\n",
    "fairness_analysis(mf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
